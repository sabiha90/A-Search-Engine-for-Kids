{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, numpy, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk as nl\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_data = pd.read_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "final_data.text = final_data.text.apply(lambda x: str(x).lower())\n",
    "final_data.text = final_data.text.apply(lambda x: x.translate(str.maketrans('','', string.punctuation)))\n",
    "final_data.text = final_data.text.apply(lambda x: x.translate(str.maketrans('','', string.digits)))\n",
    "final_data.text = final_data.text.apply(lambda x: re.sub(\"[^a-zA-Z0-9]+\", \" \", x))\n",
    "final_data.text = final_data.text.apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split() if word not in (stop_words)]))\n",
    "final_data.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['labels'] = final_data['class'].map({'Positive':1, 'Negative':0})\n",
    "#final_data = final_data.drop(['label'],axis=1)\n",
    "final_data = final_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = final_data['labels']\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(final_data['text'], final_data['labels'], test_size=0.2, random_state=0)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "X = vectorizer.fit_transform(final_data.text).toarray() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vectors = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vectors.fit(final_data['text'])\n",
    "x_train_tfidf_words =  tfidf_vectors.transform(X_train)\n",
    "x_test_tfidf_words =  tfidf_vectors.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level tf-idf \n",
    "tfidf_vectors_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vectors_ngram.fit(final_data['text'])\n",
    "x_train_tfidf_ngram =  tfidf_vectors_ngram.transform(X_train)\n",
    "x_test_tfidf_ngram =  tfidf_vectors_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_clf(x_train,y_train):\n",
    "    clf = RandomForestClassifier(n_estimators=1000, random_state=42, min_sample_split=2, criterion = \"gini\")  \n",
    "    kf = KFold(n_splits=5)\n",
    "    kf.get_n_splits(X_train)\n",
    "    predicted_y = []\n",
    "    expected_y = []\n",
    "    i = 0\n",
    "    print(\"Evaluation on cross validated data set\\n\")\n",
    "    for train_index, test_index in kf.split(x_train, y_train):\n",
    "            cv_x_train, cv_x_test = x_train[train_index], x_train[test_index]\n",
    "            cv_y_train, cv_y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "            clf.fit(cv_x_train, cv_y_train)\n",
    "            predicted_cv = clf.predict(cv_x_test)\n",
    "            print(\"For K=\",i)\n",
    "            print(\"Accuracy:\",accuracy_score(cv_y_test,predicted_cv))\n",
    "            print(classification_report(cv_y_test,predicted_cv)) \n",
    "            i += 1\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results of Model with TFIDF word scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf_word = random_forest_clf(x_train_tfidf_words, y_train)\n",
    "\n",
    "y_pred_tf_words = clf_tfidf_word.predict(x_test_tfidf_words)\n",
    "y_pred_tf_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Accuracy score:\",accuracy_score(y_test, y_pred_tf_words))  \n",
    "print(\"\\nf1-score:\",f1_score(y_test, y_pred_tf_words)) \n",
    "print(\"\\nPrecision:\",precision_score(y_test, y_pred_tf_words)) \n",
    "print(\"\\nRecall:\",recall_score(y_test, y_pred_tf_words)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Evaluation Results of Model with TFIDF ngrams scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf_n_gram = random_forest_clf(x_train_tfidf_ngram, y_train)\n",
    "\n",
    "y_pred_tf_ngram = clf_tfidf_n_gram.predict(x_test_tfidf_ngram)\n",
    "y_pred_tf_ngram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score:\",accuracy_score(y_test, y_pred_tf_ngram))  \n",
    "print(\"\\nf1-score:\",f1_score(y_test, y_pred_tf_ngram)) \n",
    "print(\"\\nPrecision:\",precision_score(y_test, y_pred_tf_ngram)) \n",
    "print(\"\\nRecall:\",recall_score(y_test, y_pred_tf_ngram)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the texts into sequences and padding them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#import matplotlib.pyplot as plt\n",
    "np.random.seed(32)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 400000\n",
    "\n",
    "# get the raw text data\n",
    "texts_train = X_train.astype(str)\n",
    "texts_test = X_test.astype(str)\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=400000, char_level=False)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = dict((i, w) for w, i in tokenizer.word_index.items())\n",
    "index_to_word[1383]\n",
    "\" \".join([index_to_word[i] for i in sequences_test[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = [len(s) for s in sequences_train]\n",
    "print(\"average length: %0.1f\" % np.mean(seq_lens))\n",
    "print(\"max length: %d\" % max(seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "# pad sequences with 0s\n",
    "x_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of data test tensor:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = train_y\n",
    "#y_test = test_y\n",
    "\n",
    "train_y = to_categorical(np.asarray(y_train))\n",
    "test_y =  to_categorical(np.asarray(y_test))\n",
    "#y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy\n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW shallow model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "Activation function: Softmax\n",
    "Optimizer: Adam\n",
    "Metric: Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D,GlobalAveragePooling2D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D, Dropout\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = 2\n",
    "\n",
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "#embedding_layer = Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(sequence_input)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "#conv_layer = Convolution1D(512, 3, activation=\"relu\")(embedding_layer)\n",
    "average = GlobalAveragePooling1D()(embedded_sequences)\n",
    "\n",
    "dense_layer = Dense(512,activation='softmax')(average)\n",
    "drop_layer = Dropout(0.2)(dense_layer)\n",
    "#dense_layer = Dense(256,activation='softmax')(drop_layer)\n",
    "\n",
    "predictions = Dense(2, activation='softmax')(drop_layer)\n",
    "\n",
    "c_bow_model_1 = Model(sequence_input, predictions)\n",
    "c_bow_model_1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['acc'])\n",
    "c_bow_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.xception.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_bow_model_1.fit(x_train, train_y,\n",
    "          validation_split=0.2,epochs=6, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test_cb_1 = c_bow_model_1.predict(x_test)\n",
    "print(\"test auc:\", roc_auc_score(y_test,output_test_cb_1[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test_cb_1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"test auc:\", roc_auc_score(y_test,output_test_cb_1[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes_cb_1 = output_test_cb_1.argmax(axis=-1) \n",
    "y_classes_cb_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print(\"f1-score:\", precision_score(y_test,y_classes_cb_1) )\n",
    "print(\"precision score:\", precision_score(y_test,y_classes_cb_1))\n",
    "print(\"recall score:\", recall_score(y_test,y_classes_cb_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "Activation function: Softmax\n",
    "Optimizer: Adam\n",
    "Metric: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = 2\n",
    "\n",
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "#embedding_layer = Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(sequence_input)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "#conv_layer = Convolution1D(512, 3, activation=\"relu\")(embedding_layer)\n",
    "average = GlobalAveragePooling1D()(embedded_sequences)\n",
    "\n",
    "dense_layer = Dense(512,activation='softmax')(average)\n",
    "drop_layer = Dropout(0.2)(dense_layer)\n",
    "#dense_layer = Dense(256,activation='softmax')(drop_layer)\n",
    "\n",
    "predictions = Dense(2, activation='softmax')(drop_layer)\n",
    "\n",
    "c_bow_model_2 = Model(sequence_input, predictions)\n",
    "c_bow_model_2.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd', metrics=['acc'])\n",
    "c_bow_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "epochs = 10\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.xception.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_bow_model_2.fit(x_train, train_y,\n",
    "          validation_split=0.2,epochs=6, batch_size=200, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test_cb_2 = c_bow_model_2.predict(x_test)\n",
    "print(\"test auc:\", roc_auc_score(y_test,output_test_cb_2[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes_cb_2 = output_test_cb_2.argmax(axis=-1) \n",
    "y_classes_cb_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print(\"f1-score:\", precision_score(y_test,y_classes_cb_2) )\n",
    "print(\"precision score:\", precision_score(y_test,y_classes_cb_2))\n",
    "print(\"recall score:\", recall_score(y_test,y_classes_cb_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D,GlobalAveragePooling2D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D, Dropout\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = 2\n",
    "\n",
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(sequence_input)\n",
    "#embedded_sequences = embedding_layer(sequence_input)\n",
    "conv_layer = Conv1D(512, 3, activation=\"relu\")(embedding_layer)\n",
    "average = GlobalAveragePooling1D()(conv_layer)\n",
    "\n",
    "dense_layer = Dense(512,activation='relu')(average)\n",
    "drop_layer = Dropout(0.2)(dense_layer)\n",
    "dense_layer = Dense(256,activation='softmax')(drop_layer)\n",
    "drop_layer = Dropout(0.2)(dense_layer)\n",
    "\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(dense_layer)\n",
    "\n",
    "cnn_model = Model(sequence_input, predictions)\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['acc'])\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.xception.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(x_train, train_y,\n",
    "          validation_split=0.2,epochs=6, batch_size=200, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test_cnn = cnn_model.predict(x_test)\n",
    "print(\"test auc:\", roc_auc_score(y_test,output_test_cnn[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_classes_cnn = output_test_cnn.argmax(axis=-1) \n",
    "y_classes_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"f1 score:\", f1_score(y_test,y_classes_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(\"precision score:\", precision_score(y_test,y_classes_cnn))\n",
    "print(\"recall score:\", recall_score(y_test,y_classes_cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the features back to text to determine the true and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for f in x_test:\n",
    " #   if f[0]!=0:\n",
    "  #      print(index_to_word[f[0]])\n",
    "import pandas as pd\n",
    "rows, cols = x_test.shape\n",
    "fin = []\n",
    "count = 0\n",
    "test_after = []\n",
    "for i,k in zip(range(rows),y_classes):\n",
    "        test_after.append([(\" \".join([index_to_word[j] for j in sequences_test[i] if j<330000 and j!=0])),k])\n",
    "df = pd.DataFrame(test_after,columns = ['text_after','pred_label'])\n",
    "df['text_prev'] = test_text.values\n",
    "df['true_label'] = y_test.values\n",
    "df['index'] = test_text.index\n",
    "df\n",
    "#test_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = pd.read_csv(\"final_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['original_data'] = [data_new['text'].iloc[i] for i in df['index']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url'] =  [data_new['url'].iloc[i] for i in df['index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_after[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['pred_label'].map({1:'Positive', 0:'Negative'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_search = df[['original_data','class','url','index']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
